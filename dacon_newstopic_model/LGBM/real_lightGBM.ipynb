{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"real_lightGBM.ipynb","provenance":[],"authorship_tag":"ABX9TyO0QNO8RlKcALNfBY9FKTBF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"2oaPKyWyQKhk"},"source":["# **1. 데이터 및 라이브러리 불러오기**\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import re\n","import json\n","import os\n","import tqdm\n","\n","from konlpy.tag import Okt\n","from konlpy.tag import Mecab\n","mecab = Mecab('C:\\mecab\\mecab-ko-dic') # 윈도우에서 설치해서 사전 위치 지정함\n","\n","import sklearn\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import log_loss, accuracy_score,f1_score\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.ensemble import RandomForestClassifier\n","\n","train=pd.read_csv('open/train.csv')\n","test=pd.read_csv('open/test.csv')\n","sample_submission=pd.read_csv('open/sample_submission.csv')\n","\n","# **2. 데이터 EDA**\n","\n","train.head(2)\n","\n","test.head(2)\n","\n","sample_submission.head(6)\n","\n","#데이터 구조 파악\n","print(train.shape)\n","print(test.shape)\n","print(sample_submission.shape)\n","\n","#심각한 불균형 데이터임을 알 수 있습니다.\n","train.label.value_counts(sort=False)/len(train)\n","\n","length=train['과제명'].astype(str).apply(len)\n","plt.hist(length, bins=50, alpha=0.5, color='r', label='word')\n","plt.title('histogram of length of task_name')\n","plt.figure(figsize=(12, 5))\n","plt.boxplot(length, labels=['counts'], showmeans=True)\n","print('과제명 길이 최댓값: {}'.format(np.max(length)))\n","print('과제명 길이 최솟값: {}'.format(np.min(length)))\n","print('과제명 길이 평균값: {}'.format(np.mean(length)))\n","print('과제명 길이 중간값: {}'.format(np.median(length)))\n","\n","length=train['요약문_연구목표'].astype(str).apply(len)\n","plt.hist(length, bins=50, alpha=0.5, color='r', label='word')\n","plt.title('histogram of length of summary_object')\n","plt.figure(figsize=(12, 5))\n","plt.boxplot(length, labels=['counts'], showmeans=True)\n","print('요약문_연구목표 길이 최댓값: {}'.format(np.max(length)))\n","print('요약문_연구목표 길이 최솟값: {}'.format(np.min(length)))\n","print('요약문_연구목표 길이 평균값: {}'.format(np.mean(length)))\n","print('요약문_연구목표 길이 중간값: {}'.format(np.median(length)))\n","\n","length=train['요약문_연구내용'].astype(str).apply(len)\n","plt.hist(length, bins=50, alpha=0.5, color='r', label='word')\n","plt.title('histogram of length of summary_content')\n","plt.figure(figsize=(12, 5))\n","plt.boxplot(length, labels=['counts'], showmeans=True)\n","print('요약문_연구내용 길이 최댓값: {}'.format(np.max(length)))\n","print('요약문_연구내용 길이 최솟값: {}'.format(np.min(length)))\n","print('요약문_연구내용 길이 평균값: {}'.format(np.mean(length)))\n","print('요약문_연구내용 길이 중간값: {}'.format(np.median(length)))\n","\n","length=train['요약문_기대효과'].astype(str).apply(len)\n","plt.hist(length, bins=50, alpha=0.5, color='r', label='word')\n","plt.title('histogram of length of summary_effect')\n","plt.figure(figsize=(12, 5))\n","plt.boxplot(length, labels=['counts'], showmeans=True)\n","print('요약문_기대효과 길이 최댓값: {}'.format(np.max(length)))\n","print('요약문_기대효과 길이 최솟값: {}'.format(np.min(length)))\n","print('요약문_기대효과 길이 평균값: {}'.format(np.mean(length)))\n","print('요약문_기대효과 길이 중간값: {}'.format(np.median(length)))\n","\n","# **3. 데이터 전처리**\n","\n","#해당 baseline 에서는 과제명 columns만 활용했습니다.\n","#다채로운 변수 활용법으로 성능을 높여주세요!\n","train = train[['과제명','label']]\n","test = test[['과제명']]\n","\n","train.head(2)\n","\n","test.head(2)\n","\n","#1. re.sub 한글 및 공백을 제외한 문자 제거\n","#2. okt 객체를 활용해 형태소 단위로 나눔\n","#3. remove_stopwords로 불용어 제거 \n","\n","def preprocessing(text, analyzer, remove_stopwords=False, stop_words=[]):\n","    text=re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ]\",\"\", text)\n","    if analyzer=='okt':\n","        analyzer = Okt()\n","        word_text=analyzer.morphs(text, stem=True)\n","    elif analyzer=='mecab':\n","        analyzer = Mecab('C:\\mecab\\mecab-ko-dic')\n","        word_text=analyzer.morphs(text)\n","    if remove_stopwords:\n","        word_review=[token for token in word_text if not token in stop_words]\n","    return word_review\n","\n","## Mecab\n","\n","stop_words=['은','는','이','가', '하','아','것','들','의','있','되','수','보','주','등','한']\n","clean_train_text=[]\n","clean_test_text=[]\n","\n","#시간이 많이 걸립니다.\n","for text in tqdm.tqdm(train['과제명']):\n","    try:\n","        clean_train_text.append(preprocessing(text, 'mecab', remove_stopwords=True, stop_words=stop_words))\n","    except:\n","        clean_train_text.append([])\n","    \n","\n","for text in tqdm.tqdm(test['과제명']):\n","    if type(text) == str:\n","        clean_test_text.append(preprocessing(text, 'mecab', remove_stopwords=True, stop_words=stop_words))\n","    else:\n","        clean_test_text.append([])\n","\n","len(clean_train_text)\n","\n","len(clean_test_text)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","#tokenizer 인자에는 list를 받아서 그대로 내보내는 함수를 넣어줍니다. 또한 소문자화를 하지 않도록 설정해야 에러가 나지 않습니다.\n","vectorizer = CountVectorizer(tokenizer = lambda x: x, lowercase=False)\n","train_features=vectorizer.fit_transform(clean_train_text)\n","test_features=vectorizer.transform(clean_test_text)\n","#test데이터에 fit_transform을 할 경우 data leakage에 해당합니다\n","\n","train_features\n","\n","test_features\n","\n","# **4. 모델링**\n","\n","train_features = train_features.astype('float32')\n","test_features = test_features.astype('float32')\n","\n","from lightgbm import LGBMClassifier\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import log_loss, accuracy_score, f1_score\n","\n","skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","folds=[]\n","for train_idx, valid_idx in skf.split(train_features, train['label']):\n","    folds.append((train_idx, valid_idx))\n","\n","# random.seed(42)\n","lgb_models={}\n","f1 = np.zeros(5)\n","for fold in range(5):\n","    print(f'================================== Fold: {fold+1} =====================================')\n","    trn_idx, val_idx = folds[fold]\n","\n","    X_trn = train_features[trn_idx]\n","    y_trn = train['label'][trn_idx]\n","    \n","    X_val = train_features[val_idx]\n","    y_val = train['label'][val_idx] \n","    \n","    lgb = LGBMClassifier(\n","         boost_from_average=False,\n","         is_unbalance=True,\n","         n_estimators=3000, \n","         learning_rate=0.1, \n","         max_depth=-1,\n","#          num_leaves=255,\n","#          min_child_samples=10,\n","         colsample_bytree=0.3,\n","         subsample=0.5,\n","         random_state=42\n","    )\n","    \n","    lgb.fit(X_trn, y_trn, \n","            eval_set=[(X_trn, y_trn), (X_val, y_val)], \n","            early_stopping_rounds=100,\n","            verbose=100)\n","    f1[fold] = f1_score(y_val, lgb.predict(X_val), average='macro')\n","    lgb_models[fold]=lgb\n","    print(f'Fold{fold+1} F1 score: {f1[fold]}')\n","    print(f'================================================================================\\n')\n","\n","f1.mean()\n","\n","pred = np.zeros((43576, 46))\n","for fold in range(5):\n","    pred += lgb_models[fold].predict_proba(test_features)\n","\n","res = pred.argmax(axis=1)\n","\n","# **5. 예측 및 제출**\n","\n","sample_submission['label']=res\n","\n","sample_submission.label.value_counts()\n","\n","sample_submission.to_csv('lgb_baseline_20210704.csv', index=False)"],"execution_count":null,"outputs":[]}]}